ZeroTouch Replatform: A software tool designed to automate the process of replatforming existing applications to a cloud-native architecture.
By analyzing an existing codebase, generating necessary boilerplate code, externalizing configurations, and modifying build pipelines,
ZeroTouch Replatform minimizes the need for manual intervention, making the transition to the cloud as seamless as possible.

Modernization Council: A team of architects directing the transition of legacy applications to cloud-native architecture.
The Council conducts in-depth analysis, provides strategic solutions, and ensures successful replatforming, serving as key advisors in the modernization journey.

Modernization Index: A numerical score, ranging from 1 to 100, representing the readiness of an application for transition to a cloud-native architecture.
The index is determined by assessing factors such as technical debt, cloud-native compliance, scalability, resilience, security, regulatory compliance,
integration complexity, performance requirements, business continuity needs, data migration challenges, and organizational readiness.
A higher score indicates a greater readiness for modernization and a more clsoud-ready state of the application.

import pandas as pd
from sqlalchemy import create_engine

# Establish connections to MariaDB and SQL Server
mariadb_engine = create_engine('mysql+pymysql://user:password@localhost/dbname')
sqlserver_engine = create_engine('mssql+pyodbc://user:password@localhost/dbname')

# Number of rows to read at a time
chunksize = 1000  # Reduce chunk size if dealing with large BLOBs

# Query to select all data from the table in MariaDB
query = 'SELECT * FROM tablename'

# Read data from MariaDB and write it to SQL Server in chunks
for chunk in pd.read_sql_query(query, mariadb_engine, chunksize=chunksize):
    # Convert BLOB columns to the correct data type, if necessary
    # chunk['blob_column'] = chunk['blob_column'].apply(lambda x: x.decode('utf-8'))
    chunk.to_sql('tablename', sqlserver_engine, if_exists='append', index=False)



import pandas as pd
from sqlalchemy import create_engine, text
from concurrent.futures import ThreadPoolExecutor

# Establish connections to MariaDB and SQL Server
mariadb_engine = create_engine('mysql+pymysql://user:password@localhost/dbname')
sqlserver_engine = create_engine('mssql+pyodbc://user:password@localhost/dbname')

# Number of rows to read at a time
chunksize = 1000  # Reduce chunk size if dealing with large BLOBs

# Number of threads to use for the data migration
num_threads = 4

# Query to select all data from the table in MariaDB
query = 'SELECT * FROM tablename'

def process_chunk(chunk):
    # Convert BLOB columns to the correct data type, if necessary
    # chunk['blob_column'] = chunk['blob_column'].apply(lambda x: x.decode('utf-8'))

    with sqlserver_engine.connect() as connection:
        connection.execute(text('SET IDENTITY_INSERT YourTableName ON'))
        chunk.to_sql('tablename', connection, if_exists='append', index=False)
        connection.execute(text('SET IDENTITY_INSERT YourTableName OFF'))

# Read data from MariaDB and write it to SQL Server in chunks
with ThreadPoolExecutor(max_workers=num_threads) as executor:
    chunks = pd.read_sql_query(query, mariadb_engine, chunksize=chunksize)
    executor.map(process_chunk, chunks)

import com.fasterxml.jackson.databind.*;
import com.fasterxml.jackson.dataformat.csv.*;
import java.nio.file.*;
import java.io.*;
import java.util.*;

public class Main {
    public static void main(String[] args) throws IOException {
        String rootDirectory = "./"; // replace with your root directory
        String inputCsv = "./input.csv"; // replace with your input CSV
        String outputCsv = "./output.csv"; // replace with your output CSV

        Map<Path, String[]> projectMapping = new HashMap<>();
        addOrphanedProjectsToMapping(rootDirectory, projectMapping);
        updateCsvLog(inputCsv, outputCsv, projectMapping);
    }

    public static void addOrphanedProjectsToMapping(String rootDirectory, Map<Path, String[]> projectMapping) throws IOException {
        Files.walk(Paths.get(rootDirectory))
            .filter(path -> path.toString().endsWith(".csproj") || path.toString().endsWith(".vbproj"))
            .forEach(projectPath -> {
                try {
                    if (!projectMapping.containsValue(projectPath)) {
                        Files.walk(projectPath.getParent())
                            .filter(srcPath -> srcPath.toString().endsWith(".cs") || srcPath.toString().endsWith(".vb"))
                            .forEach(srcPath -> projectMapping.put(srcPath, new String[]{"Unknown", projectPath.toString()}));
                    }
                } catch (IOException e) {
                    throw new UncheckedIOException(e);
                }
            });
    }

    public static void updateCsvLog(String inputCsv, String outputCsv, Map<Path, String[]> projectMapping) throws IOException {
        CsvMapper mapper = new CsvMapper();
        CsvSchema schema = CsvSchema.emptySchema().withHeader();
        ObjectReader oReader = mapper.readerFor(Map.class).with(schema);

        List<Map<String,String>> allLines = oReader.<Map<String,String>>readValues(new File(inputCsv)).readAll();

        CsvSchema outputSchema = schema.withColumn("filename").withColumn("solution_file").withColumn("project_file");
        ObjectWriter oWriter = mapper.writer(outputSchema);

        try (FileWriter outputWriter = new FileWriter(outputCsv)) {
            for (Map<String,String> line : allLines) {
                Path filePath = Paths.get(line.get("fullpath"));
                String[] projectInfo = projectMapping.getOrDefault(filePath, new String[]{"Unknown", "Unknown"});
                line.put("filename", line.get("filename"));
                line.put("solution_file", projectInfo[0]);
                line.put("project_file", projectInfo[1]);
                oWriter.writeValue(outputWriter, line);
            }
        }
    }
}

import org.w3c.dom.*;
import javax.xml.parsers.*;
import java.io.*;

public class Main {
    public static void main(String[] args) {
        String filePath = "path_to_your_csproj_file"; // replace with your .csproj file path
        System.out.println(parseCsproj(filePath));
    }

    public static String parseCsproj(String filePath) {
        try {
            File inputFile = new File(filePath);
            DocumentBuilderFactory dbFactory = DocumentBuilderFactory.newInstance();
            DocumentBuilder dBuilder = dbFactory.newDocumentBuilder();
            Document doc = dBuilder.parse(inputFile);
            doc.getDocumentElement().normalize();

            NodeList nList = doc.getElementsByTagName("AssemblyName");

            if (nList.getLength() > 0) {
                Node node = nList.item(0);
                if (node.getNodeType() == Node.ELEMENT_NODE) {
                    Element element = (Element) node;
                    return element.getTextContent();
                }
            }
        } catch (Exception e) {
            return "Error while parsing the csproj file: " + e.getMessage();
        }
        return "AssemblyName tag not found in the csproj file.";
    }
}


import com.fasterxml.jackson.databind.MappingIterator;
import com.fasterxml.jackson.dataformat.csv.CsvMapper;
import com.fasterxml.jackson.dataformat.csv.CsvSchema;
import java.nio.file.Paths;
import java.nio.file.Path;
import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;

public class Main {

    public static void main(String[] args) {
        List<String> file_paths = new ArrayList<String>();
        try {
            CsvMapper mapper = new CsvMapper();
            CsvSchema schema = CsvSchema.emptySchema().withHeader();  // Use first row as header
            MappingIterator<Map<String, String>> iterator = mapper.readerFor(Map.class)
                    .with(schema)
                    .readValues(new File("your_file.csv"));  // replace with your file name
            while (iterator.hasNext()) {
                Map<String, String> row = iterator.next();
                file_paths.add(row.get("FilePath"));  // Replace "FilePath" with your actual column name
            }
        } catch (IOException e) {
            e.printStackTrace();
        }

        String common_root = findCommonRoot(file_paths);
        System.out.println("Common root: " + common_root);
    }

    public static String findCommonRoot(List<String> paths) {
        // Normalize paths (ensure all separators are '/')
        List<String> normalized_paths = new ArrayList<String>();
        for (String path : paths) {
            normalized_paths.add(path.replace("\\", "/"));
        }

        // Find common prefix
        String common_prefix = normalized_paths.get(0);
        for (String path : normalized_paths) {
            while (!path.startsWith(common_prefix)) {
                common_prefix = common_prefix.substring(0, common_prefix.length() - 1);
            }
        }

        // Ensure common prefix is a valid path (ends at a directory boundary)
        Path path = Paths.get(common_prefix);
        String common_root = path.getParent().toString();

        return common_root;
    }
}
